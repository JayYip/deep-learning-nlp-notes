{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Colab Practice\n这个notebook大部分代码来自: https://www.tensorflow.org/tutorials/keras/basic_text_classification"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"! pip install tensorflow-gpu==2.0.0-alpha0","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tensorflow-gpu==2.0.0-alpha0\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n\u001b[K    100% |████████████████████████████████| 332.1MB 79kB/s  eta 0:00:01 1% |▋                               | 6.1MB 68.9MB/s eta 0:00:05    25% |████████                        | 83.2MB 54.1MB/s eta 0:00:05████                  | 145.7MB 43.0MB/s eta 0:00:05    44% |██████████████▎                 | 147.8MB 10.9MB/s eta 0:00:17    44% |██████████████▍                 | 148.9MB 11.2MB/s eta 0:00:17    52% |████████████████▊               | 173.3MB 45.7MB/s eta 0:00:04    97% |███████████████████████████████ | 322.3MB 48.9MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\nRequirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\nCollecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n\u001b[K    100% |████████████████████████████████| 61kB 22.9MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\nCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n\u001b[K    100% |████████████████████████████████| 3.0MB 1.9MB/s ta 0:00:01176kB 11.2MB/s eta 0:00:01    82% |██████████████████████████▌     | 2.5MB 10.0MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (0.31.1)\nRequirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\nCollecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n\u001b[K    100% |████████████████████████████████| 419kB 30.0MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.20.0)\nRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.9.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (39.1.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.14.1)\nInstalling collected packages: google-pasta, tb-nightly, tf-estimator-nightly, tensorflow-gpu\nSuccessfully installed google-pasta-0.1.7 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# import相关库\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import get_file\nimport numpy as np","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 下载IMDB电影评论数据\n\n注意,在Kaggle Kernel里面numpy版本为1.16.3, `allow_pickle`的默认值被修改了, 会导致错误, 因此需要rework\n\nRef: https://github.com/keras-team/keras/pull/12714"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(path='imdb.npz', num_words=None, skip_top=0,\n              maxlen=None, seed=113,\n              start_char=1, oov_char=2, index_from=3, **kwargs):\n    \"\"\"Loads the IMDB dataset.\n    # Arguments\n        path: where to cache the data (relative to `~/.keras/dataset`).\n        num_words: max number of words to include. Words are ranked\n            by how often they occur (in the training set) and only\n            the most frequent words are kept\n        skip_top: skip the top N most frequently occurring words\n            (which may not be informative).\n        maxlen: sequences longer than this will be filtered out.\n        seed: random seed for sample shuffling.\n        start_char: The start of a sequence will be marked with this character.\n            Set to 1 because 0 is usually the padding character.\n        oov_char: words that were cut out because of the `num_words`\n            or `skip_top` limit will be replaced with this character.\n        index_from: index actual words with this index and higher.\n    # Returns\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n    # Raises\n        ValueError: in case `maxlen` is so low\n            that no input sequence could be kept.\n    Note that the 'out of vocabulary' character is only used for\n    words that were present in the training set but are not included\n    because they're not making the `num_words` cut here.\n    Words that were not seen in the training set but are in the test set\n    have simply been skipped.\n    \"\"\"\n    # Legacy support\n    if 'nb_words' in kwargs:\n        warnings.warn('The `nb_words` argument in `load_data` '\n                      'has been renamed `num_words`.')\n        num_words = kwargs.pop('nb_words')\n    if kwargs:\n        raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n\n    path = get_file(path,\n                    origin='https://s3.amazonaws.com/text-datasets/imdb.npz',\n                    file_hash='599dadb1135973df5b59232a0e9a887c')\n    with np.load(path, allow_pickle=True) as f:\n        x_train, labels_train = f['x_train'], f['y_train']\n        x_test, labels_test = f['x_test'], f['y_test']\n\n    rng = np.random.RandomState(seed)\n    indices = np.arange(len(x_train))\n    rng.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n\n    indices = np.arange(len(x_test))\n    rng.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n\n    xs = np.concatenate([x_train, x_test])\n    labels = np.concatenate([labels_train, labels_test])\n\n    if start_char is not None:\n        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n    elif index_from:\n        xs = [[w + index_from for w in x] for x in xs]\n\n    if maxlen:\n        xs, labels = _remove_long_seq(maxlen, xs, labels)\n        if not xs:\n            raise ValueError('After filtering for sequences shorter than maxlen=' +\n                             str(maxlen) + ', no sequence was kept. '\n                             'Increase maxlen.')\n    if not num_words:\n        num_words = max([max(x) for x in xs])\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters:\n    # 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x]\n              for x in xs]\n    else:\n        xs = [[w for w in x if skip_top <= w < num_words]\n              for x in xs]\n\n    idx = len(x_train)\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_test, y_test)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get data\n\n(train_data, train_labels), (test_data, test_labels) = load_data(num_words=10000)\n\nprint(train_data[0], train_labels[0])\nprint('Number of training instances: {0}, number of testing instances: {1}'.format(train_data.shape[0], test_data.shape[0]))","execution_count":4,"outputs":[{"output_type":"stream","text":"Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n17465344/17464789 [==============================] - 0s 0us/step\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\nNumber of training instances: 25000, number of testing instances: 25000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"我们看到的是词索引, 想要看到原本的词需要用词表找回原来的词语"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get vocab\nword_to_id = keras.datasets.imdb.get_word_index()\nindex_from=3\nword_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\nword_to_id[\"<PAD>\"] = 0\nword_to_id[\"<START>\"] = 1\nword_to_id[\"<UNK>\"] = 2\nid_to_word = {value:key for key,value in word_to_id.items()}\n\nprint(' '.join([id_to_word[i] for i in train_data[0]]))","execution_count":5,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n1646592/1641221 [==============================] - 0s 0us/step\n<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"接下来我们要对输入进行补全(padding)\n\n补全会导致一部分无用计算, 但是更加方便处理(思考题: 怎样减少无用计算?)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_to_id[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_to_id[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"(25000, 256)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Problem: Use tf.data to implement input pipeline\n\n# placeholder for implementing dataset\ndef create_dataset_from_tensor_slices(X, y):\n    return tf.data.Dataset.from_tensor_slices((np.array(X), np.array(y)))\n\ndef create_dataset_from_generator(X, y):\n    def create_gen():\n        for single_x, single_y in zip(X, y):\n            yield (single_x, single_y)\n    output_types = (tf.int32, tf.int32)\n    output_shapes = ([256], [])\n    return tf.data.Dataset.from_generator(create_gen, output_types=output_types, output_shapes=output_shapes)\n\ndef create_dataset_tfrecord(X, y, mode='train'):\n    file_name = '{0}.tfrecord'.format(mode)\n    \n    # serialize features\n    # WARNING: DO NOT WRITE MULTITPLE TIMES IN PRACTICE!!! IT'S SLOW!!!\n    def _int64_list_feature(value):\n        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n    def _int64_feature(value):\n        \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n    def serialize_fn(single_x, single_y):\n        feature_tuples = {'feature': _int64_list_feature(single_x), 'label': _int64_feature(single_y)}\n        example_proto = tf.train.Example(\n            features=tf.train.Features(feature=feature_tuples))\n        return example_proto.SerializeToString()\n    # write to file\n    with tf.io.TFRecordWriter(file_name) as writer:\n        for single_x, single_y in zip(X, y):\n            example = serialize_fn(single_x, single_y)\n            writer.write(example)\n            \n    # read file\n    dataset = tf.data.TFRecordDataset(file_name)\n    def parse_fn(example_proto):\n        feature_description = {'feature': tf.io.FixedLenFeature([256], tf.int64), 'label': tf.io.FixedLenFeature([], tf.int64)}\n        feature_tuple = tf.io.parse_single_example(\n            example_proto, feature_description)\n        return feature_tuple['feature'], feature_tuple['label']\n    dataset = dataset.map(parse_fn)\n    return dataset\n\n# train_dataset = create_dataset_from_generator(train_data, train_labels)\n# test_dataset = create_dataset_from_generator(test_data, test_labels)\n\ntrain_dataset = create_dataset_tfrecord(train_data, train_labels)\ntest_dataset = create_dataset_tfrecord(test_data, test_labels, mode='test')\n\ntrain_dataset = train_dataset.shuffle(10000).batch(256).prefetch(100).repeat()\ntest_dataset = test_dataset.batch(256).prefetch(100)\n    ","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 构建模型\n\n下面就是激动人心的时候了: 写一个文本分类模型!\n\n你需要改写一下下面的模型,让其准确率更高\n\n你可以尝试使用Dropout, CudnnGRU等更加fancy的方法"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Problem: Implement a custom keras layer which has the identical effects of dense, but print the mean\n#   of the variables if the mean value is greater than zero. Print for maximum 10 times.\n\n# placeholder for implementing using Functional API or Model Subclassing\nclass WeirdDense(tf.keras.layers.Layer):\n\n    def __init__(self, output_dim, activation):\n        super(WeirdDense, self).__init__()\n        self.output_dim = output_dim\n        self.activation = activation\n        self.print_times = tf.Variable(0, dtype=tf.int32, trainable=False)\n        \n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.w = self.add_weight(shape=(input_shape[-1], self.output_dim),\n                                 initializer='random_normal',\n                                 trainable=True)\n        self.b = self.add_weight(shape=(self.output_dim,),\n                                 initializer='random_normal',\n                                 trainable=True)\n    @tf.function\n    def call(self, x):\n        mean_val = tf.reduce_mean(self.w)\n        if tf.greater(mean_val, 0):\n            if tf.less_equal(self.print_times, 10):\n                tf.print(mean_val)\n                self.print_times.assign_add(1)\n\n        return_tensor = self.activation(tf.matmul(x, self.w) + self.b)\n        return return_tensor\n            \n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# input shape is the vocabulary count used for the movie reviews (10,000 words)\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(WeirdDense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.summary()","execution_count":12,"outputs":[{"output_type":"stream","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, None, 16)          160000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 16)                0         \n_________________________________________________________________\nweird_dense_1 (WeirdDense)   (None, 16)                273       \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 160,290\nTrainable params: 160,289\nNon-trainable params: 1\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"如果要使用Keras提供的训练、预测API, 你需要先compile模型, 然后调用该API"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(train_dataset,\n                    epochs=1,\n                    steps_per_epoch=100,\n                    validation_data=test_dataset,\n                    validation_steps=100,\n                    verbose=1)","execution_count":13,"outputs":[{"output_type":"stream","text":" 91/100 [==========================>...] - ETA: 0s - loss: 0.6912 - acc: 0.56832.54052e-05\n5.81007916e-05\n 97/100 [============================>.] - ETA: 0s - loss: 0.6908 - acc: 0.57829.01562744e-05\n0.000122097204\n 99/100 [============================>.] - ETA: 0s - loss: 0.6906 - acc: 0.58070.000152926776\n0.000183319906\n0.000183319906\n0.000183319906\n0.000183319906\n0.000183319906\n0.000183319906\n","name":"stdout"},{"output_type":"stream","text":"W0617 08:00:08.996987 139913903105408 training_generator.py:228] Your dataset ran out of data; interrupting training. Make sure that your dataset can generate at least `validation_steps * epochs` batches (in this case, 100 batches). You may need to use the repeat() function when building your dataset.\n","name":"stderr"},{"output_type":"stream","text":"\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r100/100 [==============================] - 4s 43ms/step - loss: 0.6905 - acc: 0.5824 - val_loss: 0.6685 - val_acc: 0.7057\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}